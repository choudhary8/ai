{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a97b003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bf0d0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to tokens\n",
    "\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "tokenizer=tiktoken.encoding_for_model('gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2bf587f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='The cat sat on a mat'\n",
    "\n",
    "tokens=tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aa72b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62cb5b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix size => (50000, 400)\n",
      "total parameters => 20,000,000\n"
     ]
    }
   ],
   "source": [
    "# generating our own embedding matrix\n",
    "\n",
    "\n",
    "word_embeddings=np.random.randn(50000,embedding_dimensions)*0.2\n",
    "\n",
    "print(f\"matrix size => {word_embeddings.shape}\")\n",
    "print(f\"total parameters => {word_embeddings.shape[0]*word_embeddings.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a6cc46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding embedding for our text\n",
    "\n",
    "embeddings=[]\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    embeddings.append(word_embeddings[tokens[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0a72c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding calculation function\n",
    "\n",
    "def positional_embeddings_cal(tokens_len, embedding_dimensions):\n",
    "    positional_embeddings=np.random.randn(tokens_len,embedding_dimensions)\n",
    "    for pos in range(tokens_len):\n",
    "        for i in range(embedding_dimensions):\n",
    "            if(i%2==0):\n",
    "                positional_embeddings[pos][i]=np.sin(pos/10000**(i/embedding_dimensions))\n",
    "            else:\n",
    "                positional_embeddings[pos][i]=np.cos(pos/10000**((i-1)/embedding_dimensions))\n",
    "    return positional_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0896d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape: (6, 400)\n",
      "First token vector: [0.15985043 0.9803748  0.0248639  1.39522085 0.06549021]\n"
     ]
    }
   ],
   "source": [
    "# calculating final embeddings=embeddings+positional_embeddings\n",
    "\n",
    "positional_embeddings=positional_embeddings_cal(len(tokens),embedding_dimensions)\n",
    "\n",
    "final_embeddings=embeddings+positional_embeddings\n",
    "\n",
    "print(\"Final embeddings shape:\", final_embeddings.shape)\n",
    "print(\"First token vector:\", final_embeddings[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dc3412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of heads and number of tokens in sentence\n",
    "\n",
    "heads=8\n",
    "tokens_size=len(tokens)\n",
    "head_dim=int(embedding_dimensions/heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "16aa3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will divide the final_embeddings for each head\n",
    "\n",
    "\n",
    "\n",
    "# final_embeddings=final_embeddings.reshape(tokens_size,heads,head_dim)\n",
    "\n",
    "# print(f\"dimensions of embeddings for multi-heads => {final_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fa1b1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k=400\n",
    "d_v=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2fb8724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing w_q, w_k, w_v for multihead\n",
    "\n",
    "w_q=np.random.randn(embedding_dimensions,d_k)*0.2\n",
    "w_k=np.random.randn(embedding_dimensions,d_k)*0.2\n",
    "w_v=np.random.randn(embedding_dimensions,d_v)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a38286b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting embeddings for matmul\n",
    "\n",
    "# final_embeddings=np.swapaxes(final_embeddings,0,1)\n",
    "\n",
    "# print(f\"dimensions of embeddings after swapping => {final_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77fb5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimesions of Q, K, V : ((6, 400), (6, 400), (6, 400))\n"
     ]
    }
   ],
   "source": [
    "# calculating Q, K, V for each head\n",
    "\n",
    "Q=final_embeddings@w_q\n",
    "K=final_embeddings@w_k\n",
    "V=final_embeddings@w_v\n",
    "\n",
    "print(f'dimesions of Q, K, V : {Q.shape, K.shape, V.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "12b106bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimesions of Q, K, V : ((8, 6, 50), (8, 6, 50), (8, 6, 50))\n"
     ]
    }
   ],
   "source": [
    "# concating Q,K,V for attention calculation\n",
    "\n",
    "Q=Q.reshape(tokens_size,heads,head_dim)\n",
    "K=K.reshape(tokens_size,heads,head_dim)\n",
    "V=V.reshape(tokens_size,heads,head_dim)\n",
    "\n",
    "Q=Q.transpose(1,0,2)\n",
    "K=K.transpose(1,0,2)\n",
    "V=V.transpose(1,0,2)\n",
    "\n",
    "\n",
    "print(f'dimesions of Q, K, V : {Q.shape, K.shape, V.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "021fa3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    for head in range(x.shape[0]):\n",
    "        for i in range(x.shape[1]):\n",
    "            s=0\n",
    "            for j in range(x.shape[2]):\n",
    "                s+=np.exp(x[head][i][j])\n",
    "            for j in range(x.shape[2]):\n",
    "                x[head][i][j]=np.exp(x[head][i][j])/s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e4ae286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights Shape: (8, 6, 6)\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "Contextual embeddings after multi-head attention calculation :\n",
      "\n",
      "Final output Shape: (6, 400) as d_v=400 before spliting\n",
      "\n",
      "Token 976 ('The')\n",
      "   -> First 10 values : [ 12.905 -16.903  10.272  -2.295  -1.195  -3.487  -6.019   4.227 -10.514\n",
      "  -9.059]\n",
      "\n",
      "Token 9059 (' cat')\n",
      "   -> First 10 values : [  8.097 -18.872   8.876   5.371  -0.063   3.554 -10.938  13.415  -6.129\n",
      " -12.051]\n",
      "\n",
      "Token 10139 (' sat')\n",
      "   -> First 10 values : [ 14.671 -13.537   9.13   -3.22   -3.885  -1.065  -7.857   5.308 -10.129\n",
      "  -9.751]\n",
      "\n",
      "Token 402 (' on')\n",
      "   -> First 10 values : [  2.86  -18.651   6.334   6.741  -0.623   4.745 -16.825  13.486  -2.456\n",
      "  -8.696]\n",
      "\n",
      "Token 261 (' a')\n",
      "   -> First 10 values : [ 14.243 -17.972  11.785  -3.772  -3.609   1.147 -10.608   6.465 -10.13\n",
      "  -7.749]\n",
      "\n",
      "Token 2450 (' mat')\n",
      "   -> First 10 values : [  3.637 -11.634   8.981   9.056  -4.331   3.431 -20.526  18.158  -4.744\n",
      "  -5.725]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculaitng attention\n",
    "\n",
    "K=K.transpose(0,2,1)\n",
    "\n",
    "similarity_score=Q@K\n",
    "\n",
    "scaled_dot_product=(similarity_score/np.sqrt(d_k))\n",
    "\n",
    "attention_weights=softmax(scaled_dot_product)\n",
    "\n",
    "print(f\"Attention weights Shape: {attention_weights.shape}\\n\")\n",
    "\n",
    "# attention_weights=attention_weights.transpose(1,0,2)\n",
    "# attention_weights=attention_weights.reshape(attention_weights.shape[0],-1)/8\n",
    "\n",
    "contextual_emdeddings=attention_weights@V\n",
    "\n",
    "\n",
    "contextual_emdeddings=contextual_emdeddings.transpose(1,0,2)\n",
    "contextual_emdeddings=contextual_emdeddings.reshape(contextual_emdeddings.shape[0],-1)\n",
    "\n",
    "\n",
    "W_o = np.random.randn(heads * head_dim, embedding_dimensions) * 0.2\n",
    "final_output = contextual_emdeddings @ W_o\n",
    "\n",
    "# print(\"Attention weights (after multi-head concat and softmax) for every token in sentence:\")\n",
    "# print(f\"Shape: {attention_weights.shape}\\n\")\n",
    "\n",
    "# # Display as a table\n",
    "# print(\"           \", end=\"\")\n",
    "# for token in tokens:\n",
    "#     print(f\"{f\"'{tokenizer.decode([token])}'\":>8}\", end=\"\")\n",
    "# print(\"    | Sum\")\n",
    "# print(\"-\" * 70)\n",
    "\n",
    "# for i, token in enumerate(tokens):\n",
    "#     print(f\"{f\"'{tokenizer.decode([token])}'\":>8}   \", end=\"\")\n",
    "#     for j in range(len(tokens)):\n",
    "#         print(f\"{attention_weights[i,j]:>8.3f}\", end=\"\")\n",
    "#     print(f\"  | {attention_weights[i].sum():.3f}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "print('Contextual embeddings after multi-head attention calculation :\\n')\n",
    "print(f\"Final output Shape: {final_output.shape} as d_v=400 before spliting\")\n",
    "print()\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"Token {token} ('{tokenizer.decode([token])}')\")\n",
    "    print(f\"   -> First 10 values : {final_output[i][:10].round(3)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
