{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af23de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef1d96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to tokens\n",
    "\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "tokenizer=tiktoken.encoding_for_model('gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ca643d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens - [976, 9059, 10139, 402, 261, 2450]\n",
      "976 => 'The' \n",
      "9059 => ' cat' \n",
      "10139 => ' sat' \n",
      "402 => ' on' \n",
      "261 => ' a' \n",
      "2450 => ' mat' \n"
     ]
    }
   ],
   "source": [
    "text='The cat sat on a mat'\n",
    "\n",
    "tokens=tokenizer.encode(text)\n",
    "\n",
    "print(f\"Tokens - {tokens}\")\n",
    "for i in range(len(tokens)):\n",
    "    print(f\"{tokens[i]} => '{tokenizer.decode([tokens[i]])}' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b96263b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimensions=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1e44bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix size => (50000, 768)\n",
      "total parameters => 38,400,000\n"
     ]
    }
   ],
   "source": [
    "# generating our own embedding matrix\n",
    "\n",
    "\n",
    "word_embeddings=np.random.randn(50000,embedding_dimensions)*0.2\n",
    "\n",
    "print(f\"matrix size => {word_embed.shape}\")\n",
    "print(f\"total parameters => {word_embed.shape[0]*word_embed.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec9c6c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "80346d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embedding for generated tokens\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "word_embeds_glove=api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "35d8fb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for first token - \n",
      "\n",
      "'The' ===> [-0.24727    0.78307    0.44702   -0.084627   0.71603    0.25502\n",
      "  0.27146    0.31966   -1.0295    -0.47782    0.90986    0.0031653\n",
      "  0.32648    0.38458    0.61674    0.049444   0.11239    0.46098\n",
      " -0.5804     0.77743    0.95716    0.80546    0.30824   -0.031789\n",
      "  0.49038    0.17329   -0.45901    0.0175    -0.22298    0.060455\n",
      " -0.53695    1.2004    -0.068347   0.11316   -0.30841    0.40903\n",
      " -0.68957   -0.35439   -0.37194    0.8153     0.47602   -0.068007\n",
      "  0.070446  -0.44186    0.25154   -0.46211    0.3426    -0.93956\n",
      "  0.4344     0.60238   -0.55132    0.16768    0.073117   0.72065\n",
      " -0.18476   -2.3391    -0.49756    0.31342    1.4492     0.88318\n",
      " -0.72643   -0.4188     0.55796    0.23447   -0.1621     0.75175\n",
      "  0.76326   -0.50907    0.25204    0.57319   -0.2553     0.13437\n",
      " -0.30616   -0.86615    0.87236    0.0071972 -0.68176   -0.12665\n",
      " -0.36754    0.093175   0.12012    0.78489   -0.63707    0.024234\n",
      " -0.57392    0.24726   -0.11193   -0.23344    0.21587    0.75304\n",
      " -0.25362   -0.066268  -0.18849   -0.38368   -0.47027    0.55613\n",
      "  0.22954    0.44426    0.53048    0.39723  ]\n"
     ]
    }
   ],
   "source": [
    "print('Embedding for first token - ')\n",
    "print()\n",
    "print(f\"'{tokenizer.decode([tokens[0]])}' ===> {word_embeds_glove[tokens[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e32f04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding embedding for our text\n",
    "\n",
    "embeddings=[]\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    embeddings.append(word_embeddings[tokens[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8cce1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding calculation function\n",
    "\n",
    "def positional_embeddings_cal(tokens_len, embedding_dimensions):\n",
    "    positional_embeddings=np.random.randn(tokens_len,embedding_dimensions)\n",
    "    for pos in range(tokens_len):\n",
    "        for i in range(embedding_dimensions):\n",
    "            if(i%2==0):\n",
    "                positional_embeddings[pos][i]=np.sin(pos/10000**(i/embedding_dimensions))\n",
    "            else:\n",
    "                positional_embeddings[pos][i]=np.cos(pos/10000**((i-1)/embedding_dimensions))\n",
    "    return positional_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "70cfdb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape: (6, 100)\n",
      "First token vector: [-0.19237136  1.67402152 -0.07199606  1.28888887 -0.24032851]\n"
     ]
    }
   ],
   "source": [
    "# calculating final embeddings=embeddings+positional_embeddings\n",
    "\n",
    "positional_embeddings=positional_embeddings_cal(len(tokens),embedding_dimensions)\n",
    "\n",
    "final_embeddings=embeddings+positional_embeddings\n",
    "\n",
    "print(\"Final embeddings shape:\", final_embeddings.shape)\n",
    "print(\"First token vector:\", final_embeddings[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "11721f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entering attention part\n",
    "# defining w_q, w_k, w_v - for now taking as random matrix as currently not having learned matrix\n",
    "\n",
    "d_k=70\n",
    "d_v=50\n",
    "\n",
    "w_q=np.random.randn(embedding_dimensions,d_k)*0.2\n",
    "w_k=np.random.randn(embedding_dimensions,d_k)*0.2\n",
    "w_v=np.random.randn(embedding_dimensions,d_v)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b186df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding Q, K, V\n",
    "\n",
    "Q=final_embeddings@w_q\n",
    "K=final_embeddings@w_k\n",
    "V=final_embeddings@w_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2d084e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        s=0\n",
    "        for j in range(x.shape[1]):\n",
    "            s+=np.exp(x[i][j])\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i][j]=np.exp(x[i][j])/s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d963ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (after softmax) for every token in sentence:\n",
      "Shape: (6, 6)\n",
      "\n",
      "              'The'  ' cat'  ' sat'   ' on'    ' a'  ' mat'    | Sum\n",
      "----------------------------------------------------------------------\n",
      "   'The'      0.034   0.021   0.069   0.088   0.248   0.539  | 1.000\n",
      "  ' cat'      0.049   0.017   0.084   0.121   0.230   0.499  | 1.000\n",
      "  ' sat'      0.162   0.031   0.063   0.085   0.162   0.497  | 1.000\n",
      "   ' on'      0.177   0.068   0.091   0.139   0.225   0.300  | 1.000\n",
      "    ' a'      0.091   0.060   0.091   0.317   0.287   0.154  | 1.000\n",
      "  ' mat'      0.058   0.047   0.081   0.296   0.405   0.112  | 1.000\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Contextual embeddings after attention calculation :\n",
      "Shape: (6, 50) as d_v=50\n",
      "\n",
      "Token 976 ('The')\n",
      "   -> First 10 values : [ 0.867 -0.536  1.151 -1.726  0.066 -0.299 -0.733  0.816  0.068 -1.924]\n",
      "\n",
      "Token 9059 (' cat')\n",
      "   -> First 10 values : [ 0.925 -0.578  1.106 -1.637  0.06  -0.314 -0.711  0.762  0.128 -1.978]\n",
      "\n",
      "Token 10139 (' sat')\n",
      "   -> First 10 values : [ 1.17  -0.639  1.037 -1.503 -0.068 -0.256 -0.551  0.628  0.207 -2.023]\n",
      "\n",
      "Token 402 (' on')\n",
      "   -> First 10 values : [ 1.391 -0.7    0.992 -1.179 -0.128 -0.189 -0.577  0.542  0.448 -2.201]\n",
      "\n",
      "Token 261 (' a')\n",
      "   -> First 10 values : [ 1.351 -0.764  1.    -1.04   0.039 -0.315 -0.717  0.619  0.554 -2.353]\n",
      "\n",
      "Token 2450 (' mat')\n",
      "   -> First 10 values : [ 1.283 -0.652  1.135 -1.067  0.054 -0.245 -0.856  0.789  0.519 -2.293]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating attention\n",
    "\n",
    "similarity_score=Q@K.T\n",
    "scaled_dot_product=similarity_score/np.sqrt(d_k)\n",
    "attention_weights=softmax(scaled_dot_product)\n",
    "\n",
    "contextual_emdeddings=attention_weights@V\n",
    "\n",
    "print(\"Attention weights (after softmax) for every token in sentence:\")\n",
    "print(f\"Shape: {attention_weights.shape}\\n\")\n",
    "\n",
    "# Display as a table\n",
    "print(\"           \", end=\"\")\n",
    "for token in tokens:\n",
    "    print(f\"{f\"'{tokenizer.decode([token])}'\":>8}\", end=\"\")\n",
    "print(\"    | Sum\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{f\"'{tokenizer.decode([token])}'\":>8}   \", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        print(f\"{attention_weights[i,j]:>8.3f}\", end=\"\")\n",
    "    print(f\"  | {attention_weights[i].sum():.3f}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print('Contextual embeddings after attention calculation :')\n",
    "print(f\"Shape: {contextual_emdeddings.shape} as d_v=50\")\n",
    "print()\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"Token {token} ('{tokenizer.decode([token])}')\")\n",
    "    print(f\"   -> First 10 values : {contextual_emdeddings[i][:10].round(3)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
