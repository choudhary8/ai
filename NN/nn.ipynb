{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4f87d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a neural networks for OR, AND & XOR gate\n",
    "import numpy as np\n",
    "\n",
    "input_layer_size=3\n",
    "hidden_layer_size=4\n",
    "output_layer_size=1\n",
    "\n",
    "w_ih=np.random.randn(input_layer_size,hidden_layer_size)*0.2\n",
    "w_ho=np.random.randn(hidden_layer_size,output_layer_size)*0.2\n",
    "\n",
    "b_hidden=np.random.randn(1,hidden_layer_size)*0.02\n",
    "b_output=np.random.randn(1,output_layer_size)*0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9d9c439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def activation_derivative(x):\n",
    "    # s=activation(x)\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "7be0e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x,w_ih,w_ho,b_hidden,b_output):\n",
    "    hidden=activation(x@w_ih+b_hidden)\n",
    "    output=activation(hidden@w_ho+b_output)\n",
    "    return hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1b722351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer -> [[0.48459653 0.47373166 0.61426449 0.66887607]]\n",
      "output layer -> 0.5616366840156664\n"
     ]
    }
   ],
   "source": [
    "x=np.array([1,0,1])\n",
    "hidden,output=forward(x,w_ih,w_ho,b_hidden,b_output)\n",
    "print(f\"hidden layer -> {hidden}\")\n",
    "print(f\"output layer -> {output[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b351e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossCal(z_pred,z_actual):\n",
    "    return 0.5*(z_actual-z_pred)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "02917895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation(w_ho, w_ih, x, y, z_pred, z_actual, b_hidden, b_output, learning_rate):\n",
    "    grad_output=(z_pred-z_actual)\n",
    "    \n",
    "    grad_act2=activation_derivative(z_pred)\n",
    "    \n",
    "    grad_b_output=grad_output*grad_act2\n",
    "    grad_w_ho=y.T@(grad_output*grad_act2)\n",
    "    grad_y=grad_output*grad_act2@w_ho.T\n",
    "    grad_act1=activation_derivative(y)\n",
    "\n",
    "    grad_b_hidden=grad_y*grad_act1\n",
    "    \n",
    "    grad_w_ih=x.T@(grad_y*grad_act1)\n",
    "    \n",
    "    \n",
    "    b_output-=(grad_b_output*learning_rate)\n",
    "    b_hidden-=(grad_b_hidden*learning_rate)\n",
    "    w_ho-=(grad_w_ho*learning_rate)\n",
    "    w_ih-=(grad_w_ih*learning_rate)\n",
    "    \n",
    "    return w_ih, w_ho, b_hidden, b_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8fb90874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output layer [0 0 1]-> 0.0 & correct output is [0]\n",
      "output layer [0 1 1]-> 1.0 & correct output is [1]\n",
      "output layer [1 0 1]-> 1.0 & correct output is [1]\n",
      "output layer [1 1 1]-> 0.0 & correct output is [0]\n",
      "output layer [0 0 2]-> 0.0 & correct output is [0]\n",
      "output layer [0 1 2]-> 0.0 & correct output is [0]\n",
      "output layer [1 0 2]-> 0.0 & correct output is [0]\n",
      "output layer [1 1 2]-> 1.0 & correct output is [1]\n",
      "output layer [0 0 3]-> 0.0 & correct output is [0]\n",
      "output layer [0 1 3]-> 1.0 & correct output is [1]\n",
      "output layer [1 0 3]-> 1.0 & correct output is [1]\n",
      "output layer [1 1 3]-> 1.0 & correct output is [1]\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1],[0,0,2],[0,1,2],[1,0,2],[1,1,2],[0,0,3],[0,1,3],[1,0,3],[1,1,3]])\n",
    "Y=np.array([[0],[1],[1],[0],[0],[0],[0],[1],[0],[1],[1],[1]])\n",
    "epoch=10000\n",
    "learning_rate=0.5\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(len(X)):\n",
    "        y_hidden,y_output=forward(X[j:j+1],w_ih,w_ho,b_hidden,b_output)\n",
    "\n",
    "        w_ih, w_ho, b_hidden, b_output=backpropogation(w_ho, w_ih, X[j:j+1], y_hidden, y_output, Y[j:j+1], b_hidden, b_output, learning_rate)\n",
    "     \n",
    "\n",
    "for j in range(len(X)):\n",
    "    hidden,output=forward(X[j:j+1],w_ih,w_ho,b_hidden,b_output)\n",
    "    # print(f\"hidden layer -> {hidden}\")\n",
    "    print(f\"output layer {X[j]}-> {output[0][0].round()} & correct output is {Y[j]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
